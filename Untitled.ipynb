{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a947488d-8396-408f-9453-04d0ccbc1029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: C:\\AIDC\\travel\\temp_models_betatest\n",
      "ğŸ”¹ TensorFlow Version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, TimeDistributed\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# === ì„¤ì • ===\n",
    "# í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©\n",
    "BASE_DIR = os.getcwd()\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"temp_models_betatest\")\n",
    "\n",
    "# ì„ì‹œ ëª¨ë¸ ì €ì¥ í´ë” ìƒì„±\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "print(f\"ğŸ“‚ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {MODEL_DIR}\")\n",
    "print(f\"ğŸ”¹ TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# --- ê³µí†µ í•˜ì´í¼íŒŒë¼ë¯¸í„° (ë”ë¯¸ìš©) ---\n",
    "VOCAB_SIZE = 1000  # ë‹¨ì–´ ì‚¬ì „ í¬ê¸°\n",
    "MAX_LEN = 50       # ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´\n",
    "EMBEDDING_DIM = 64 # ì„ë² ë”© ì°¨ì›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b016cc4a-b39d-48d9-aa0f-07d85e1ec048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\AIDC\\travel\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1189d04-64d9-477b-bde8-12b4ee725d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8b6c698-9a7b-495e-91ca-62dffadd2d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\AIDC\\travel\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: C:\\AIDC\\travel\\advanced_models_sota\n",
      "ğŸ”¹ Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    ElectraConfig, ElectraForTokenClassification, ElectraForSequenceClassification, # M1, M2ìš©\n",
    "    BartConfig, BartForConditionalGeneration, # M3ìš©\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "# === ì„¤ì • ===\n",
    "BASE_DIR = os.getcwd()\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"advanced_models_sota\")\n",
    "\n",
    "# í´ë” ìƒì„±\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "print(f\"ğŸ“‚ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {MODEL_DIR}\")\n",
    "\n",
    "# CPU/GPU ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ”¹ Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35f4c52a-ee17-4cf6-b44c-7fc462f0ca17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [M1] KoELECTRA (NER) ìƒì„± ë° ì €ì¥ ---\n",
      "âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: C:\\AIDC\\travel\\advanced_models_sota\\koelectra_ner\n",
      "\n",
      "--- [M1] ë¡œë“œ ë° ì¶”ë¡  í…ŒìŠ¤íŠ¸ ---\n",
      "   ã„´ ì¶œë ¥ í˜•íƒœ: torch.Size([1, 50, 5])\n",
      "   ã„´ ì •ìƒ ì‘ë™ í™•ì¸.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- [M1] KoELECTRA (NER) ìƒì„± ë° ì €ì¥ ---\")\n",
    "\n",
    "# 1. ì„¤ì • (Config) ìƒì„± - ì‹¤ì œ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ëŒ€ì‹  êµ¬ì¡°ë§Œ ê°€ì ¸ì˜´ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)\n",
    "# ì‹¤ì œ ì‚¬ìš©ì‹œëŠ” 'monologg/koelectra-base-v3-discriminator' ë“±ì„ ì‚¬ìš©\n",
    "config_m1 = ElectraConfig(\n",
    "    vocab_size=30000,\n",
    "    hidden_size=64, # í…ŒìŠ¤íŠ¸ìš©ì´ë¼ ì‘ê²Œ ì„¤ì • (ì›ë˜ëŠ” 768)\n",
    "    num_hidden_layers=2, # ë ˆì´ì–´ ìˆ˜ë„ ì¶•ì†Œ\n",
    "    num_labels=5, # íƒœê·¸ ê°œìˆ˜ (B-Hotel, I-Price ë“±)\n",
    "    max_position_embeddings=128\n",
    ")\n",
    "\n",
    "# 2. ëª¨ë¸ ì´ˆê¸°í™” (Random Weights)\n",
    "model_m1 = ElectraForTokenClassification(config_m1).to(device)\n",
    "\n",
    "# 3. ì €ì¥ (Hugging Face í‘œì¤€ ë°©ì‹)\n",
    "m1_path = os.path.join(MODEL_DIR, \"koelectra_ner\")\n",
    "model_m1.save_pretrained(m1_path)\n",
    "print(f\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {m1_path}\")\n",
    "\n",
    "# --- Load & Test ---\n",
    "print(\"\\n--- [M1] ë¡œë“œ ë° ì¶”ë¡  í…ŒìŠ¤íŠ¸ ---\")\n",
    "loaded_m1 = ElectraForTokenClassification.from_pretrained(m1_path).to(device)\n",
    "loaded_m1.eval()\n",
    "\n",
    "# ë”ë¯¸ ì…ë ¥ (Batch: 1, Length: 50)\n",
    "dummy_input_ids = torch.randint(0, 30000, (1, 50)).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = loaded_m1(dummy_input_ids)\n",
    "    logits = outputs.logits # (1, 50, 5)\n",
    "\n",
    "print(f\"   ã„´ ì¶œë ¥ í˜•íƒœ: {logits.shape}\")\n",
    "print(\"   ã„´ ì •ìƒ ì‘ë™ í™•ì¸.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cac999c-5638-4aa5-b2dd-45b777bed1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [M2] KoELECTRA Small (Sentiment) ìƒì„± ë° ì €ì¥ ---\n",
      "âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: C:\\AIDC\\travel\\advanced_models_sota\\koelectra_sentiment\n",
      "\n",
      "--- [M2] ë¡œë“œ ë° ì¶”ë¡  í…ŒìŠ¤íŠ¸ ---\n",
      "   ã„´ ì˜ˆì¸¡ í™•ë¥  ë¶„í¬: [0.33222324 0.33774316 0.3300336 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- [M2] KoELECTRA Small (Sentiment) ìƒì„± ë° ì €ì¥ ---\")\n",
    "\n",
    "# 1. ì„¤ì • (3-Class: ê¸ì •/ë¶€ì •/ë³´ë¥˜)\n",
    "config_m2 = ElectraConfig(\n",
    "    vocab_size=30000,\n",
    "    hidden_size=64,\n",
    "    num_hidden_layers=2,\n",
    "    num_labels=3, # 3ì¤‘ ë¶„ë¥˜\n",
    "    max_position_embeddings=128\n",
    ")\n",
    "\n",
    "# 2. ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model_m2 = ElectraForSequenceClassification(config_m2).to(device)\n",
    "\n",
    "# 3. ì €ì¥\n",
    "m2_path = os.path.join(MODEL_DIR, \"koelectra_sentiment\")\n",
    "model_m2.save_pretrained(m2_path)\n",
    "print(f\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {m2_path}\")\n",
    "\n",
    "# --- Load & Test ---\n",
    "print(\"\\n--- [M2] ë¡œë“œ ë° ì¶”ë¡  í…ŒìŠ¤íŠ¸ ---\")\n",
    "loaded_m2 = ElectraForSequenceClassification.from_pretrained(m2_path).to(device)\n",
    "loaded_m2.eval()\n",
    "\n",
    "dummy_input_ids = torch.randint(0, 30000, (1, 50)).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = loaded_m2(dummy_input_ids)\n",
    "    probs = torch.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "print(f\"   ã„´ ì˜ˆì¸¡ í™•ë¥  ë¶„í¬: {probs[0].cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9b4918b-1571-42e4-9a60-118c77f93083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [M3] KoBART (Summarizer) ìƒì„± ë° ì €ì¥ ---\n",
      "âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: C:\\AIDC\\travel\\advanced_models_sota\\kobart_summary\n",
      "\n",
      "--- [M3] ë¡œë“œ ë° ìƒì„± í…ŒìŠ¤íŠ¸ ---\n",
      "   ã„´ ìƒì„±ëœ ìš”ì•½ë¬¸ í† í° ID: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- [M3] KoBART (Summarizer) ìƒì„± ë° ì €ì¥ ---\")\n",
    "\n",
    "# 1. ì„¤ì • (Encoder-Decoder êµ¬ì¡°)\n",
    "config_m3 = BartConfig(\n",
    "    vocab_size=30000,\n",
    "    d_model=64,       # hidden size\n",
    "    encoder_layers=2,\n",
    "    decoder_layers=2,\n",
    "    max_position_embeddings=128\n",
    ")\n",
    "\n",
    "# 2. ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model_m3 = BartForConditionalGeneration(config_m3).to(device)\n",
    "\n",
    "# 3. ì €ì¥\n",
    "m3_path = os.path.join(MODEL_DIR, \"kobart_summary\")\n",
    "model_m3.save_pretrained(m3_path)\n",
    "print(f\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {m3_path}\")\n",
    "\n",
    "# --- Load & Test ---\n",
    "print(\"\\n--- [M3] ë¡œë“œ ë° ìƒì„± í…ŒìŠ¤íŠ¸ ---\")\n",
    "loaded_m3 = BartForConditionalGeneration.from_pretrained(m3_path).to(device)\n",
    "loaded_m3.eval()\n",
    "\n",
    "dummy_input_ids = torch.randint(0, 30000, (1, 100)).to(device) # ê¸´ ë¬¸ì¥ ì…ë ¥\n",
    "\n",
    "with torch.no_grad():\n",
    "    # generate() í•¨ìˆ˜ë¥¼ í†µí•´ ìš”ì•½ë¬¸ ìƒì„±\n",
    "    summary_ids = loaded_m3.generate(dummy_input_ids, max_length=20)\n",
    "\n",
    "print(f\"   ã„´ ìƒì„±ëœ ìš”ì•½ë¬¸ í† í° ID: {summary_ids.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4c63200-0f98-4ff1-9e4c-c1c8ba443d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [M4] N-BEATS Style (Forecaster) ìƒì„± ë° ì €ì¥ ---\n",
      "âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: C:\\AIDC\\travel\\advanced_models_sota\\nbeats_forecast.pth\n",
      "\n",
      "--- [M4] ë¡œë“œ ë° ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ---\n",
      "   ã„´ ì…ë ¥ ë°ì´í„°: torch.Size([1, 30])\n",
      "   ã„´ ì˜ˆì¸¡ê°’: 0.05511678010225296\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- [M4] N-BEATS Style (Forecaster) ìƒì„± ë° ì €ì¥ ---\")\n",
    "\n",
    "# 1. ê°„ë‹¨í•œ N-BEATS ìŠ¤íƒ€ì¼ ëª¨ë¸ ì •ì˜ (PyTorch)\n",
    "class SimpleNBeats(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=64):\n",
    "        super(SimpleNBeats, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim) # Trend ì˜ˆì¸¡\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch, Time_Steps) - ì—¬ê¸°ì„  ë‹¨ìˆœí™”\n",
    "        out = self.relu(self.fc1(x))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        return self.fc3(out)\n",
    "\n",
    "# 2. ëª¨ë¸ ì´ˆê¸°í™”\n",
    "TIME_STEPS = 30 # ê³¼ê±° 30ì¼\n",
    "FORECAST_HORIZON = 1 # ë‹¤ìŒë‚  ì˜ˆì¸¡\n",
    "model_m4 = SimpleNBeats(input_dim=TIME_STEPS, output_dim=FORECAST_HORIZON).to(device)\n",
    "\n",
    "# 3. ì €ì¥ (PyTorch ë°©ì‹)\n",
    "m4_path = os.path.join(MODEL_DIR, \"nbeats_forecast.pth\")\n",
    "torch.save(model_m4.state_dict(), m4_path)\n",
    "print(f\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {m4_path}\")\n",
    "\n",
    "# --- Load & Test ---\n",
    "print(\"\\n--- [M4] ë¡œë“œ ë° ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ---\")\n",
    "loaded_m4 = SimpleNBeats(input_dim=TIME_STEPS, output_dim=FORECAST_HORIZON).to(device)\n",
    "loaded_m4.load_state_dict(torch.load(m4_path))\n",
    "loaded_m4.eval()\n",
    "\n",
    "# ë”ë¯¸ ì…ë ¥ (Batch: 1, 30ì¼ì¹˜ ë°ì´í„°)\n",
    "dummy_input = torch.randn(1, TIME_STEPS).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = loaded_m4(dummy_input)\n",
    "\n",
    "print(f\"   ã„´ ì…ë ¥ ë°ì´í„°: {dummy_input.shape}\")\n",
    "print(f\"   ã„´ ì˜ˆì¸¡ê°’: {pred.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4397331-82cb-478e-973a-4149a1c45b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [Pre] Tokenizer ìƒì„± ---\n",
      "âœ… Tokenizer ì €ì¥ ì™„ë£Œ: C:\\AIDC\\travel\\advanced_models_sota\\tokenizer\n",
      "   ã„´ í† í°í™” ê²°ê³¼: ['ì—¬', '##í–‰', 'ê°€', '##ê³ ', 'ì‹¶', '##ì–´', '##ìš”']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- [Pre] Tokenizer ìƒì„± ---\")\n",
    "\n",
    "# ì‹¤ì œë¡œëŠ” 'monologg/koelectra...' ë“±ì„ ë‹¤ìš´ë¡œë“œ ë°›ì§€ë§Œ\n",
    "# ì—¬ê¸°ì„œëŠ” í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì„ì‹œ í† í¬ë‚˜ì´ì € ì €ì¥ êµ¬ì¡°ë§Œ í‰ë‚´ëƒ…ë‹ˆë‹¤.\n",
    "# (ì¸í„°ë„· ì—°ê²°ì´ ìˆë‹¤ë©´ AutoTokenizer.from_pretrained('bert-base-multilingual-cased') ë“±ì„ ì‚¬ìš© ê¶Œì¥)\n",
    "\n",
    "try:\n",
    "    # ì¸í„°ë„·ì´ ì—°ê²°ë˜ì–´ ìˆë‹¤ë©´ ì‹¤ì œ í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ í›„ ì €ì¥\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "    tokenizer_path = os.path.join(MODEL_DIR, \"tokenizer\")\n",
    "    tokenizer.save_pretrained(tokenizer_path)\n",
    "    print(f\"âœ… Tokenizer ì €ì¥ ì™„ë£Œ: {tokenizer_path}\")\n",
    "    \n",
    "    # ë¡œë“œ í…ŒìŠ¤íŠ¸\n",
    "    loaded_tok = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    tokens = loaded_tok.tokenize(\"ì—¬í–‰ ê°€ê³  ì‹¶ì–´ìš”\")\n",
    "    print(f\"   ã„´ í† í°í™” ê²°ê³¼: {tokens}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"âš ï¸ ì¸í„°ë„· ì—°ê²°ì´ ì—†ê±°ë‚˜ ëª¨ë¸ IDê°€ ì˜ëª»ë˜ì–´ í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨.\")\n",
    "    print(\"   ì‹¤ë¬´ì—ì„œëŠ” ì‚¬ë‚´ë§ ë°˜ì… ì ˆì°¨ë¥¼ í†µí•´ 'vocab.txt' ë“±ì„ í™•ë³´í•´ì•¼ í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77f7df3-4a50-4efc-8963-2c8fb535a5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02f95d8-7e73-455f-aaf7-11c3fe1fcd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# ==========================================\n",
    "# [ì„¤ì •]\n",
    "# ==========================================\n",
    "MY_API_KEY = \"AIzaSyBRQJ1uVE5UVvB6jO6HvRhB5LTfWncsqN8\"  # ë³¸ì¸ì˜ API í‚¤\n",
    "MODEL_NAME = \"gemini-2.5-flash\"\n",
    "genai.configure(api_key=MY_API_KEY, transport=\"rest\")\n",
    "\n",
    "# â˜… ì¤‘ìš”: ì•„ê¹Œ ë‹¤ìš´ë°›ì€ Popplerì˜ bin í´ë” ê²½ë¡œë¥¼ ì—¬ê¸°ì— ë¶™ì—¬ë„£ìœ¼ì„¸ìš” ( \\ ëŒ€ì‹  / ì‚¬ìš© ê¶Œì¥ )\n",
    "# ì˜ˆì‹œ: r\"C:\\Start\\poppler-24.02.0\\Library\\bin\"\n",
    "POPPLER_BIN_PATH = r\"C:\\poppler\\Library\\bin\"\n",
    "\n",
    "try:\n",
    "    from docx2pdf import convert as docx_to_pdf_tool\n",
    "except ImportError:\n",
    "    docx_to_pdf_tool = None\n",
    "\n",
    "# ==========================================\n",
    "# [ìŠ¤í‚¤ë§ˆ ì •ì˜] ì—¬í–‰ ìƒí’ˆ ë°ì´í„° êµ¬ì¡°\n",
    "# ==========================================\n",
    "\n",
    "class BasicInfo(typing.TypedDict):\n",
    "    product_type: str | None       # êµ­ë‚´ìƒí’ˆ / í•´ì™¸ìƒí’ˆ\n",
    "    is_flight_included: bool | None\n",
    "    is_vat_included: bool | None\n",
    "\n",
    "class LocationInfo(typing.TypedDict):\n",
    "    country: str | None\n",
    "    city: str | None\n",
    "    departure_port: str | None\n",
    "\n",
    "class EventPeriod(typing.TypedDict):\n",
    "    available_days: list[str]\n",
    "\n",
    "class ProductInfo(typing.TypedDict):\n",
    "    product_name: str | None\n",
    "    event_period: EventPeriod\n",
    "\n",
    "class MetaInfoHotel(typing.TypedDict):\n",
    "    check_in_out: str | None\n",
    "    website: str | None\n",
    "\n",
    "class Hotel(typing.TypedDict):\n",
    "    name_kr: str | None\n",
    "    description: str | None        # ë“±ê¸‰(3ì„±/4ì„±) ì •ë³´ë‚˜ ìœ„ì¹˜ ì„¤ëª… ë“±\n",
    "    meta_info: MetaInfoHotel\n",
    "\n",
    "class MetaInfoGolf(typing.TypedDict):\n",
    "    detail_info: str | None\n",
    "    website: str | None\n",
    "\n",
    "class GolfCourse(typing.TypedDict):\n",
    "    name_kr: str | None\n",
    "    operation_info: str | None\n",
    "    meta_info: MetaInfoGolf\n",
    "\n",
    "class TouristSpot(typing.TypedDict):\n",
    "    name: str | None\n",
    "\n",
    "class Details(typing.TypedDict):\n",
    "    inclusions: list[str]          # í¬í•¨ì‚¬í•­\n",
    "    exclusions: list[str]          # ë¶ˆí¬í•¨ì‚¬í•­\n",
    "    others: str | None             # ê¸°íƒ€\n",
    "    is_insurance_included: bool | None\n",
    "    is_guide_included: bool | None\n",
    "    special_notes: list[str]       # ì‡¼í•‘, ì„ íƒê´€ê´‘, ìœ ì˜ì‚¬í•­\n",
    "\n",
    "class AiContent(typing.TypedDict):\n",
    "    body_text: str | None          # AIê°€ ì‘ì„±í•œ ë§ˆì¼€íŒ…ìš© ìš”ì•½ ë³¸ë¬¸\n",
    "\n",
    "class FlightInfo(typing.TypedDict):\n",
    "    airline: str | None\n",
    "    flight_number: str | None      # LJ357 ë“±\n",
    "    departure_time: str | None\n",
    "    arrival_time: str | None\n",
    "\n",
    "class PriceInfo(typing.TypedDict):\n",
    "    departure_date: str\n",
    "    night_count: int\n",
    "    day_count: int\n",
    "    group_size: int\n",
    "    price_adult: int\n",
    "    status: str\n",
    "\n",
    "# â˜… ë©”ì¸ ìŠ¤í‚¤ë§ˆ (ìµœì¢… ì¶œë ¥)\n",
    "class TravelProductSchema(typing.TypedDict):\n",
    "    basic_info: BasicInfo\n",
    "    location_info: LocationInfo\n",
    "    product_info: ProductInfo\n",
    "    hotels: list[Hotel]             # â˜… í•µì‹¬: ë¦¬ìŠ¤íŠ¸ë¡œ ì •ì˜ (1ê°œë“  3ê°œë“  ë‹¤ ìˆ˜ìš©)\n",
    "    golf_courses: list[GolfCourse]  # â˜… í•µì‹¬: ë¦¬ìŠ¤íŠ¸ë¡œ ì •ì˜\n",
    "    tourist_spots: list[TouristSpot]\n",
    "    details: Details\n",
    "    ai_content: AiContent\n",
    "    flight_info: FlightInfo\n",
    "    price_info: list[PriceInfo]\n",
    "\n",
    "# ê°€ê²©í‘œ ì „ìš© ìŠ¤í‚¤ë§ˆ\n",
    "class PriceListSchema(typing.TypedDict):\n",
    "    prices: list[PriceInfo]\n",
    "\n",
    "# ==========================================\n",
    "# [í´ë˜ìŠ¤] ë§ŒëŠ¥ ì—¬í–‰ ë°ì´í„° ì²˜ë¦¬ê¸°\n",
    "# ==========================================\n",
    "class UniversalTravelAI:\n",
    "    def __init__(self):\n",
    "        self.model = genai.GenerativeModel(MODEL_NAME)\n",
    "\n",
    "    # 429 ì—ëŸ¬ ë°©ì§€ìš© ì¬ì‹œë„ ë¡œì§\n",
    "    def _generate_with_retry(self, content, config, retries=3):\n",
    "        for i in range(retries):\n",
    "            try:\n",
    "                return self.model.generate_content(\n",
    "                    content, generation_config=config, request_options={\"timeout\": 120}\n",
    "                )\n",
    "            except Exception as e:\n",
    "                # í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ëŒ€ê¸°\n",
    "                if \"429\" in str(e) or \"quota\" in str(e).lower():\n",
    "                    wait_time = (i + 1) * 15\n",
    "                    print(f\"   âš ï¸ Quota Exceeded. Retrying in {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else: raise e\n",
    "        return None\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œê¸° (ëª¨ë“  íŒŒì¼ -> í…ìŠ¤íŠ¸)\n",
    "    # ---------------------------------------------------------\n",
    "    def _extract_text_content(self, file_path):\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        print(f\"   ğŸ“„ [Text Extractor] Reading {ext} file...\")\n",
    "        try:\n",
    "            if ext == '.pdf':\n",
    "                text = \"\"\n",
    "                with pdfplumber.open(file_path) as pdf:\n",
    "                    for page in pdf.pages:\n",
    "                        page_text = page.extract_text()\n",
    "                        if page_text: text += page_text + \"\\n\"\n",
    "                return text\n",
    "            elif ext in ['.docx', '.doc']:\n",
    "                doc = Document(file_path)\n",
    "                return \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "            elif ext in ['.xlsx', '.xls']:\n",
    "                xls = pd.read_excel(file_path, sheet_name=None)\n",
    "                text = \"\"\n",
    "                for sheet, df in xls.items():\n",
    "                    text += f\"--- Sheet: {sheet} ---\\n\"\n",
    "                    # tabulate ì—†ìœ¼ë©´ to_stringìœ¼ë¡œ ëŒ€ì²´ (ì—ëŸ¬ ë°©ì§€)\n",
    "                    try: text += df.to_markdown(index=False) + \"\\n\"\n",
    "                    except: text += df.to_string(index=False) + \"\\n\"\n",
    "                return text\n",
    "            elif ext == '.txt':\n",
    "                with open(file_path, 'r', encoding='utf-8') as f: return f.read()\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Text Extraction Failed: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2. ì´ë¯¸ì§€ ë³€í™˜ê¸° (Visionìš©)\n",
    "    # ---------------------------------------------------------\n",
    "    def _convert_to_images(self, file_path):\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        print(f\"   ğŸ–¼ï¸ [Image Converter] Processing {ext} file...\")\n",
    "\n",
    "        pdf_path = file_path\n",
    "        # Docx -> PDF ë³€í™˜\n",
    "        if ext in ['.docx', '.doc']:\n",
    "            try:\n",
    "                from docx2pdf import convert\n",
    "                pdf_path = os.path.splitext(file_path)[0] + \".pdf\"\n",
    "                convert(file_path, pdf_path)\n",
    "            except: return []\n",
    "\n",
    "        # PDF -> Images\n",
    "        if pdf_path.lower().endswith('.pdf'):\n",
    "            try:\n",
    "                # Docker/Linux í™˜ê²½ì—ì„œëŠ” poppler_path ì œê±° í•„ìš”\n",
    "                return convert_from_path(pdf_path, dpi=300, poppler_path=POPPLER_BIN_PATH)\n",
    "            except: return []\n",
    "        return []\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3. ë©”ì¸ í”„ë¡œì„¸ìŠ¤ (ìë™ ë¶„ê¸° ì²˜ë¦¬)\n",
    "    # ---------------------------------------------------------\n",
    "    def analyze(self, product_file, price_file=None):\n",
    "        print(f\"\\nğŸš€ Analysis Started\")\n",
    "\n",
    "        # [Step 1] ìƒí’ˆ ì •ë³´ ì¶”ì¶œ (í…ìŠ¤íŠ¸ ëª¨ë“œ)\n",
    "        # í…ìŠ¤íŠ¸ ëª¨ë“œëŠ” í† í°ì´ ì €ë ´í•˜ê³ , URL/ì „í™”ë²ˆí˜¸/íŠ¹ìˆ˜ë¬¸ì ì¸ì‹ë¥ ì´ 100%ì…ë‹ˆë‹¤.\n",
    "        product_text = self._extract_text_content(product_file)\n",
    "\n",
    "        if not product_text:\n",
    "            return {\"error\": \"ìƒí’ˆ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨\"}\n",
    "\n",
    "        # Gemini í˜¸ì¶œ\n",
    "        product_data = self._call_gemini_product(product_text)\n",
    "\n",
    "        # [Step 2] ê°€ê²© ì •ë³´ ì¶”ì¶œ\n",
    "        # ê°€ê²© íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒí’ˆ íŒŒì¼ì„ ê°€ê²© ë¶„ì„ ëŒ€ìƒìœ¼ë¡œ ì‚¼ìŒ\n",
    "        price_source = price_file if price_file else product_file\n",
    "        price_ext = os.path.splitext(price_source)[1].lower()\n",
    "        price_list = []\n",
    "\n",
    "        # ì „ëµ A: ì—‘ì…€/í…ìŠ¤íŠ¸ íŒŒì¼ -> í…ìŠ¤íŠ¸ ëª¨ë“œë¡œ ë¶„ì„ (í‘œ êµ¬ì¡° ìœ ì§€ë¨)\n",
    "        if price_ext in ['.xlsx', '.xls', '.csv', '.txt']:\n",
    "            print(\"   -> [Strategy A] Excel/Text detected.\")\n",
    "            raw_text = self._extract_text_content(price_source)\n",
    "            price_list = self._call_gemini_price_text(raw_text)\n",
    "\n",
    "        # ì „ëµ B: PDF/ì´ë¯¸ì§€ íŒŒì¼ -> Vision ëª¨ë“œë¡œ ë¶„ì„ (ë³µì¡í•œ ë ˆì´ì•„ì›ƒ ì¸ì‹)\n",
    "        else:\n",
    "            print(\"   -> [Strategy B] PDF/Image detected.\")\n",
    "            images = self._convert_to_images(price_source)\n",
    "            if images:\n",
    "                price_list = self._call_gemini_price_vision(images)\n",
    "            else:\n",
    "                # ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ ëª¨ë“œë¡œ ë¹„ìƒ ì „í™˜\n",
    "                print(\"   âš ï¸ Vision failed. Fallback to Text Analysis.\")\n",
    "                raw_text = self._extract_text_content(price_source)\n",
    "                price_list = self._call_gemini_price_text(raw_text)\n",
    "\n",
    "        # [Step 3] ë³‘í•©\n",
    "        if not product_data: product_data = {}\n",
    "        product_data['price_info'] = price_list\n",
    "\n",
    "        return product_data\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 4. Gemini í˜¸ì¶œ ë©”ì„œë“œ (í”„ë¡¬í”„íŠ¸ ê°•í™”)\n",
    "    # ---------------------------------------------------------\n",
    "    def _call_gemini_product(self, text):\n",
    "        \"\"\"ìƒí’ˆ ìƒì„¸ ì •ë³´ ì¶”ì¶œ (ë‹¨ì¼/ë‹¤ì¤‘ ì˜µì…˜ ëª¨ë‘ ëŒ€ì‘)\"\"\"\n",
    "\n",
    "        prompt = \"\"\"\n",
    "        You are a generic travel product data parser.\n",
    "        Analyze the text and extract details into the JSON schema perfectly.\n",
    "\n",
    "        [MAPPING RULES - CRITICAL]\n",
    "\n",
    "        1. **Multiple Options Handling (Hotels/Golf)**:\n",
    "           - If the text lists multiple options (e.g., \"3-star: A Hotel, 4-star: B Hotel\" or \"A CC, B CC, C CC\"),\n",
    "             **extract ALL of them** as separate items in the `hotels` or `golf_courses` list.\n",
    "           - Do not merge them into one string. Create a list of objects.\n",
    "\n",
    "        2. **flight_info (í•­ê³µ)**:\n",
    "           - Look for flight codes (e.g., LJ357, 7C, KE, OZ) and times.\n",
    "           - If found, fill `flight_info` and set `is_flight_included` = True.\n",
    "\n",
    "        3. **Location**:\n",
    "           - If text contains \"ì˜¤í‚¤ë‚˜ì™€\" or \"ë¯¸ì•¼ì½”ì§€ë§ˆ\", set Country=\"ì¼ë³¸\".\n",
    "\n",
    "        4. **AI Content Creation**:\n",
    "           - Read the whole text and write a summarizing marketing text in `ai_content.body_text`.\n",
    "           - It should highlight key selling points (e.g., hotel grade, golf course view).\n",
    "\n",
    "        5. **Language**:\n",
    "           - All text output MUST be in **Korean**.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            resp = self._generate_with_retry(\n",
    "                [prompt, text[:50000]],\n",
    "                genai.GenerationConfig(\n",
    "                    response_mime_type=\"application/json\",\n",
    "                    response_schema=TravelProductSchema\n",
    "                )\n",
    "            )\n",
    "            if resp:\n",
    "                data = json.loads(resp.text)\n",
    "\n",
    "                # í›„ì²˜ë¦¬: êµ­ë‚´/í•´ì™¸ ìë™ ë³´ì •\n",
    "                country = data.get(\"location_info\", {}).get(\"country\", \"\")\n",
    "                if country and any(x in country for x in [\"í•œêµ­\", \"ëŒ€í•œë¯¼êµ­\", \"ì œì£¼\", \"Korea\"]):\n",
    "                    data[\"basic_info\"][\"product_type\"] = \"êµ­ë‚´ìƒí’ˆ\"\n",
    "                else:\n",
    "                    data[\"basic_info\"][\"product_type\"] = \"í•´ì™¸ìƒí’ˆ\"\n",
    "                return data\n",
    "            return {}\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Product Info Error: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def _call_gemini_price_text(self, text):\n",
    "        \"\"\"ê°€ê²© ì •ë³´ (Text ëª¨ë“œ)\"\"\"\n",
    "        prompt = \"Extract pricing table. Rows: Date, Price, Headcount. Date format: YYYY-MM-DD.\"\n",
    "        try:\n",
    "            resp = self._generate_with_retry(\n",
    "                [prompt, text[:30000]],\n",
    "                genai.GenerationConfig(\n",
    "                    response_mime_type=\"application/json\",\n",
    "                    response_schema=PriceListSchema\n",
    "                )\n",
    "            )\n",
    "            return json.loads(resp.text).get('prices', []) if resp else []\n",
    "        except Exception: return []\n",
    "\n",
    "    def _call_gemini_price_vision(self, images):\n",
    "        \"\"\"ê°€ê²© ì •ë³´ (Vision ëª¨ë“œ)\"\"\"\n",
    "        all_prices = []\n",
    "        # í† í° ì ˆì•½ì„ ìœ„í•´ ì• 5ì¥ë§Œ ë¶„ì„ (í•„ìš”ì‹œ ì¡°ì • ê°€ëŠ¥)\n",
    "        for img in images[:5]:\n",
    "            try:\n",
    "                time.sleep(2) # Rate Limit ë°©ì§€\n",
    "                resp = self._generate_with_retry(\n",
    "                    [\"Extract price table. Output JSON only.\", img],\n",
    "                    genai.GenerationConfig(\n",
    "                        response_mime_type=\"application/json\",\n",
    "                        response_schema=PriceListSchema\n",
    "                    )\n",
    "                )\n",
    "                if resp:\n",
    "                    items = json.loads(resp.text).get('prices', [])\n",
    "                    all_prices.extend(items)\n",
    "            except: continue\n",
    "        return all_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e46d51-40e7-4765-a343-0f5e0790025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# ==========================================\n",
    "# [ì„¤ì •]\n",
    "# ==========================================\n",
    "MY_API_KEY = \"AIzaSyBRQJ1uVE5UVvB6jO6HvRhB5LTfWncsqN8\"  # ë³¸ì¸ì˜ API í‚¤\n",
    "MODEL_NAME = \"gemini-2.5-flash\"\n",
    "genai.configure(api_key=MY_API_KEY, transport=\"rest\")\n",
    "\n",
    "# â˜… ì¤‘ìš”: ì•„ê¹Œ ë‹¤ìš´ë°›ì€ Popplerì˜ bin í´ë” ê²½ë¡œë¥¼ ì—¬ê¸°ì— ë¶™ì—¬ë„£ìœ¼ì„¸ìš” ( \\ ëŒ€ì‹  / ì‚¬ìš© ê¶Œì¥ )\n",
    "# ì˜ˆì‹œ: r\"C:\\Start\\poppler-24.02.0\\Library\\bin\"\n",
    "POPPLER_BIN_PATH = r\"C:\\poppler\\Library\\bin\"\n",
    "\n",
    "class TravelVisionAI:\n",
    "    def __init__(self):\n",
    "        self.model = genai.GenerativeModel(MODEL_NAME)\n",
    "        # Target Schema\n",
    "        self.target_schema = {\n",
    "            \"basic_info\": {\"product_type\": None, \"is_flight_included\": None, \"is_vat_included\": None},\n",
    "            \"location_info\": {\"country\": None, \"city\": None, \"departure_port\": None},\n",
    "            \"product_info\": {\n",
    "                \"product_name\": None,\n",
    "                \"event_period\": {\"available_days\": []}\n",
    "            },\n",
    "            \"hotels\": [{\"name_kr\": None, \"description\": None, \"meta_info\": {\"check_in_out\": None, \"website\": None}}],\n",
    "            \"golf_courses\": [{\"name_kr\": None, \"operation_info\": None, \"meta_info\": {\"detail_info\": None, \"website\": None}}],\n",
    "            \"tourist_spots\": [{\"name\": None}],\n",
    "            \"details\": {\n",
    "                \"inclusions\": [], \"exclusions\": [], \"others\": None,\n",
    "                \"is_insurance_included\": None, \"is_guide_included\": None,\n",
    "                \"special_notes\": []\n",
    "            },\n",
    "            \"ai_content\": {\"body_text\": None},\n",
    "            \"flight_info\": {\"airline\": None, \"flight_number\": None, \"departure_time\": None, \"arrival_time\": None},\n",
    "            \"price_info\": []\n",
    "        }\n",
    "\n",
    "    def _convert_to_pdf_if_needed(self, file_path):\n",
    "        \"\"\"\n",
    "        ì…ë ¥ íŒŒì¼ì´ DOCXë¼ë©´ PDFë¡œ ë³€í™˜í•˜ì—¬ ê²½ë¡œ ë°˜í™˜.\n",
    "        ì´ë¯¸ PDFë¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜.\n",
    "        \"\"\"\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "        # 1. ì´ë¯¸ PDFì¸ ê²½ìš°\n",
    "        if ext == '.pdf':\n",
    "            return file_path\n",
    "\n",
    "        # 2. DOCXì¸ ê²½ìš° -> PDF ë³€í™˜\n",
    "        elif ext in ['.docx', '.doc']:\n",
    "            print(f\"   â„¹ï¸ DOCX ê°ì§€ë¨. PDFë¡œ ë³€í™˜ ì¤‘... ({os.path.basename(file_path)})\")\n",
    "            if docx_to_pdf_tool is None:\n",
    "                print(\"   âŒ docx2pdf ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (!pip install docx2pdf)\")\n",
    "                return None\n",
    "\n",
    "            # ê°™ì€ í´ë”ì— ê°™ì€ ì´ë¦„ì˜ pdf ìƒì„±\n",
    "            output_pdf = os.path.splitext(file_path)[0] + \".pdf\"\n",
    "            try:\n",
    "                docx_to_pdf_tool(file_path, output_pdf)\n",
    "                print(f\"   âœ… ë³€í™˜ ì™„ë£Œ: {output_pdf}\")\n",
    "                return output_pdf\n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ DOCX ë³€í™˜ ì‹¤íŒ¨ (MS Wordê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•¨): {e}\")\n",
    "                return None\n",
    "\n",
    "        # 3. ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼\n",
    "        else:\n",
    "            print(f\"   âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {ext}\")\n",
    "            return None\n",
    "\n",
    "    def _file_to_images(self, file_path):\n",
    "        \"\"\"íŒŒì¼ ê²½ë¡œë¥¼ ë°›ì•„ (ë³€í™˜ í›„) ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ ë¦¬í„´\"\"\"\n",
    "\n",
    "        # 1. PDFê°€ ì•„ë‹ˆë¼ë©´ PDFë¡œ ë³€í™˜\n",
    "        pdf_path = self._convert_to_pdf_if_needed(file_path)\n",
    "\n",
    "        if not pdf_path or not os.path.exists(pdf_path):\n",
    "            return []\n",
    "\n",
    "        print(f\"   -> ì´ë¯¸ì§€ ë³€í™˜ ì¤‘: {os.path.basename(pdf_path)}\")\n",
    "        try:\n",
    "            return convert_from_path(pdf_path, dpi=400, poppler_path=POPPLER_BIN_PATH)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Poppler ì—ëŸ¬: {e}\")\n",
    "            return []\n",
    "\n",
    "    def process(self, product_file: str, price_file: str = None):\n",
    "        print(\">>> [1/3] ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘...\")\n",
    "\n",
    "        # [ìˆ˜ì •] _pdf_to_images ëŒ€ì‹  _file_to_images ì‚¬ìš© (DOCX ìë™ ì²˜ë¦¬)\n",
    "        prod_images = self._file_to_images(product_file)\n",
    "        price_images = self._file_to_images(price_file) if price_file else []\n",
    "\n",
    "        if not prod_images:\n",
    "            return {\"error\": \"ìƒí’ˆ íŒŒì¼ ë³€í™˜ ì‹¤íŒ¨\"}\n",
    "\n",
    "        print(\">>> [2/3] Gemini Vision ë¶„ì„ ì¤‘...\")\n",
    "\n",
    "        # 1. ìƒí’ˆ ì •ë³´ ì¶”ì¶œ\n",
    "        product_json = self._extract_product_info(prod_images)\n",
    "\n",
    "        # 2. ê°€ê²© ì •ë³´ ì¶”ì¶œ\n",
    "        if price_images:\n",
    "            print(\"   -> [Mode] ë³„ë„ ê°€ê²© íŒŒì¼ ë¶„ì„\")\n",
    "            target_images = price_images\n",
    "        else:\n",
    "            print(\"   -> [Mode] í†µí•© íŒŒì¼ ë¶„ì„\")\n",
    "            target_images = prod_images\n",
    "\n",
    "        price_json = self._extract_price_info(target_images)\n",
    "\n",
    "        print(\">>> [3/3] ë°ì´í„° ë³‘í•© ì¤‘...\")\n",
    "        final_data = self._merge(product_json, price_json)\n",
    "        return final_data\n",
    "\n",
    "    def _extract_product_info(self, images):\n",
    "        # [ìˆ˜ì •] product_type ë¶„ë¥˜ ë¡œì§ ì¶”ê°€ (Prompt + Python Logic)\n",
    "        prompt = f\"\"\"\n",
    "        Role: Travel Product Analyst.\n",
    "        Task: Analyze the images and extract product details into JSON.\n",
    "\n",
    "        [CRITICAL LANGUAGE RULES]\n",
    "        1. **OUTPUT VALUES MUST BE IN KOREAN.** (e.g., \"Japan\"->\"ì¼ë³¸\")\n",
    "        2. **DO NOT TRANSLATE KEYS.**\n",
    "\n",
    "        [Extraction Focus]\n",
    "        1. **Location**: Identify the `country` and `city` accurately.\n",
    "           - If `country` is South Korea, map as \"í•œêµ­\".\n",
    "        2. **Product Type Logic**:\n",
    "           - If `country` is \"í•œêµ­\" (or Jeju), set `product_type` to \"êµ­ë‚´ìƒí’ˆ\".\n",
    "           - Otherwise, set `product_type` to \"í•´ì™¸ìƒí’ˆ\".\n",
    "        3. **Flight Info**: Look specifically for flight codes (LJ, KE, TW, etc.) and times.\n",
    "        4. Flight Status: Set is_flight_included to \"í¬í•¨\" if flight codes are present, otherwise \"ë¶ˆí¬í•¨\".\n",
    "\n",
    "        [Target JSON Schema]\n",
    "        {json.dumps(self.target_schema, ensure_ascii=False)}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # ìƒí’ˆ ì •ë³´ëŠ” ì†ë„ë¥¼ ìœ„í•´ ì•ë¶€ë¶„ 5ì¥ë§Œ í™•ì¸ (ì„ íƒì‚¬í•­)\n",
    "            content = [prompt] + images[:5]\n",
    "            res = self.model.generate_content(\n",
    "                content,\n",
    "                generation_config={\"response_mime_type\": \"application/json\"},\n",
    "                request_options={\"timeout\": 120}\n",
    "            )\n",
    "            data = json.loads(res.text)\n",
    "\n",
    "            if isinstance(data, list):\n",
    "                data = data[0] if len(data) > 0 else {}\n",
    "\n",
    "            # ==========================================================\n",
    "            # [Python í›„ì²˜ë¦¬] product_type ê°•ì œ ë³´ì • ë¡œì§\n",
    "            # ==========================================================\n",
    "            country = data.get(\"location_info\", {}).get(\"country\", \"\")\n",
    "            if country:\n",
    "                if any(x in country for x in [\"í•œêµ­\", \"ëŒ€í•œë¯¼êµ­\", \"ì œì£¼\", \"Korea\"]):\n",
    "                    data[\"basic_info\"][\"product_type\"] = \"êµ­ë‚´ìƒí’ˆ\"\n",
    "                    data[\"location_info\"][\"country\"] = \"í•œêµ­\"\n",
    "                else:\n",
    "                    data[\"basic_info\"][\"product_type\"] = \"í•´ì™¸ìƒí’ˆ\"\n",
    "\n",
    "            return data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Product Extract Error: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def _extract_price_info(self, images):\n",
    "        all_prices = []\n",
    "        seen_entries = set()\n",
    "\n",
    "        print(f\"   -> ì´ {len(images)}ì¥ì˜ ê°€ê²©í‘œ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤ (í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ìº”).\")\n",
    "\n",
    "        for page_idx, full_img in enumerate(images):\n",
    "            width, height = full_img.size\n",
    "\n",
    "            # [ì¶”ê°€] í•˜ì´ë¸Œë¦¬ë“œ ì²­í¬ ìƒì„±ì „ëµ\n",
    "            # ì „ëµ 1: ì „ì²´ ì´ë¯¸ì§€ í†µì§¸ë¡œ ìŠ¤ìº” (ì¼ë°˜ì ì¸ í‘œ ëŒ€ì‘, idx=-1ë¡œ êµ¬ë¶„)\n",
    "            chunks = [(full_img, -1, \"Full Page\")]\n",
    "\n",
    "            # ì „ëµ 2: ìˆ˜ì§ 3ë“±ë¶„ ìŠ¤ìº” (ë³µì¡í•œ ì—‘ì…€ ë¤í”„ ëŒ€ì‘)\n",
    "            # ì´ë¯¸ì§€ê°€ ì¶©ë¶„íˆ ë„“ì„ ë•Œë§Œ ìˆ˜í–‰ (ì˜ˆ: ë„ˆë¹„ 1000px ì´ìƒ)\n",
    "            if width > 1000:\n",
    "                split_1 = int(width / 3)\n",
    "                split_2 = int(width * 2 / 3)\n",
    "                chunks.append((full_img.crop((0, 0, split_1, height)), 0, \"Left Column\"))\n",
    "                chunks.append((full_img.crop((split_1, 0, split_2, height)), 1, \"Middle Column\"))\n",
    "                chunks.append((full_img.crop((split_2, 0, width, height)), 2, \"Right Column\"))\n",
    "\n",
    "            print(f\"      Running Page {page_idx + 1} ({len(chunks)} Scans)...\")\n",
    "\n",
    "            for img_chunk, col_idx, position in chunks:\n",
    "\n",
    "                # [í•µì‹¬ ë¡œì§ 1] ì»¬ëŸ¼ ìœ„ì¹˜ì— ë”°ë¥¸ ê¸°ë³¸ê°’ ì„¤ì •\n",
    "                if col_idx == 0:\n",
    "                    def_night, def_day, def_group = 2, 3, 4\n",
    "                elif col_idx == 1:\n",
    "                    def_night, def_day, def_group = 3, 4, 4\n",
    "                elif col_idx == 2:\n",
    "                    def_night, def_day, def_group = 4, 5, 4\n",
    "                else:\n",
    "                    # [ì¶”ê°€] col_idx == -1 (Full Page)ì¼ ë•Œì˜ ê¸°ë³¸ê°’\n",
    "                    # ì¼ë°˜ì ì¸ ìƒí’ˆì¼ ê²½ìš° ê°€ì¥ í”í•œ íŒ¨í„´(3ë°•4ì¼, 4ì¸)ì„ ê¸°ë³¸ìœ¼ë¡œ ì¡ê±°ë‚˜,\n",
    "                    # ì´ ê°’ì€ ì‹¤ì œ íŒŒì‹±ë  ë•Œ ë¬´ì‹œë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "                    def_night, def_day, def_group = 3, 4, 4\n",
    "\n",
    "                prompt = f\"\"\"\n",
    "                Role: Travel Price Analyst.\n",
    "                Task: Extract pricing rows from this image fragment ({position}).\n",
    "\n",
    "                [CRITICAL INSTRUCTION: CSV FORMAT]\n",
    "                1. Output **CSV ONLY**. No JSON, No Markdown.\n",
    "                2. Format: `departure_date, price_adult`\n",
    "                3. **Ignore missing columns**: I will fill night/day/group programmatically.\n",
    "                4. **Date Parsing**: Convert \"12ì›” 01ì¼\" -> \"2025-12-01\" (Assume 2025 for Dec, 2026 for Jan+).\n",
    "                5. **Price Parsing**: Extract numbers only (e.g., 1,010,000 -> 1010000). Skip empty rows.\n",
    "                \"\"\"\n",
    "\n",
    "                try:\n",
    "                    res = self.model.generate_content(\n",
    "                        [prompt, img_chunk],\n",
    "                        generation_config={\n",
    "                            \"max_output_tokens\": 8192,\n",
    "                            \"temperature\": 0.0\n",
    "                        },\n",
    "                        request_options={\"timeout\": 120}\n",
    "                    )\n",
    "\n",
    "                    csv_text = res.text.strip()\n",
    "\n",
    "                    # [í•µì‹¬ ë¡œì§ 2] íŒŒì„œì— ê¸°ë³¸ê°’(defaults)ì„ ì „ë‹¬í•˜ì—¬ ê°•ì œ ì£¼ì…\n",
    "                    new_rows = self._parse_csv_to_json(\n",
    "                        csv_text,\n",
    "                        seen_entries,\n",
    "                        defaults=(def_night, def_day, def_group)\n",
    "                    )\n",
    "\n",
    "                    if new_rows:\n",
    "                        all_prices.extend(new_rows)\n",
    "                        # print(f\"         âœ… {position}: Extracted {len(new_rows)} rows.\")\n",
    "                    # else:\n",
    "                        # print(f\"         âš ï¸ {position}: No data found.\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    # print(f\"         âŒ {position} Error: {e}\")\n",
    "                    continue\n",
    "\n",
    "        # ë‚ ì§œìˆœ ì •ë ¬\n",
    "        all_prices.sort(key=lambda x: x['departure_date'])\n",
    "\n",
    "        return {\n",
    "            \"global_updates\": {\"is_vat_included\": None, \"cancellation_refund\": None},\n",
    "            \"price_info\": all_prices\n",
    "        }\n",
    "\n",
    "    def _parse_csv_to_json(self, csv_text, seen_entries, defaults):\n",
    "        \"\"\"CSVë¥¼ íŒŒì‹±í•˜ê³ , ë‚ ì§œ/ê°€ê²© ì™¸ì˜ ì •ë³´ëŠ” ì „ë‹¬ë°›ì€ defaults ê°’ìœ¼ë¡œ ì±„ì›€\"\"\"\n",
    "        rows = []\n",
    "        lines = csv_text.split('\\n')\n",
    "\n",
    "        # ì „ë‹¬ë°›ì€ ê°•ì œ ì£¼ì… ê°’ë“¤\n",
    "        def_night, def_day, def_group = defaults\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            if \"date\" in line.lower() or \"price\" in line.lower(): continue\n",
    "\n",
    "            parts = line.split(',')\n",
    "            # í”„ë¡¬í”„íŠ¸ì—ì„œ ë‚ ì§œ, ê°€ê²©ë§Œ ë‹¬ë¼ê³  í–ˆìœ¼ë¯€ë¡œ 2ê°œ ì´ìƒì´ë©´ ë¨\n",
    "            if len(parts) < 2: continue\n",
    "\n",
    "            try:\n",
    "                # 1. ë‚ ì§œ ì •ê·œí™”\n",
    "                raw_date = parts[0].strip()\n",
    "                clean_date = raw_date.replace('.', '-').replace('/', '-')\n",
    "\n",
    "                # ë‚ ì§œì— í•œê¸€ì´ë‚˜ ê³µë°±ì´ ì„ì¸ ê²½ìš° ì²˜ë¦¬ (ì˜ˆ: \"12ì›” 01ì¼\")\n",
    "                import re\n",
    "                nums = re.findall(r'\\d+', clean_date)\n",
    "                if len(nums) >= 2:\n",
    "                    month = int(nums[-2])\n",
    "                    day = int(nums[-1])\n",
    "\n",
    "                    # ì—°ë„ ì¶”ì • (10,11,12ì›”ì€ 2025ë…„, 1,2,3ì›”ì€ 2026ë…„ìœ¼ë¡œ ê°€ì •)\n",
    "                    year = 2025 if month >= 10 else 2026\n",
    "                    clean_date = f\"{year}-{month:02d}-{day:02d}\"\n",
    "                else:\n",
    "                    continue # ë‚ ì§œ ì¸ì‹ ì‹¤íŒ¨ ì‹œ ìŠ¤í‚µ\n",
    "\n",
    "                # 2. ê°€ê²© ì¶”ì¶œ (ë§ˆì§€ë§‰ ì»¬ëŸ¼ì´ ê°€ê²©ì¼ í™•ë¥  ë†’ìŒ)\n",
    "                price_str = parts[-1].strip()\n",
    "                price = int(re.sub(r'\\D', '', price_str))\n",
    "\n",
    "                # 3. ì¤‘ë³µ ì²´í¬ í‚¤\n",
    "                unique_key = f\"{clean_date}_{def_night}_{def_day}_{def_group}_{price}\"\n",
    "\n",
    "                if unique_key not in seen_entries:\n",
    "                    seen_entries.add(unique_key)\n",
    "\n",
    "                    rows.append({\n",
    "                        \"departure_date\": clean_date,\n",
    "                        \"night_count\": def_night,  # ê°•ì œ ì£¼ì…\n",
    "                        \"day_count\": def_day,      # ê°•ì œ ì£¼ì…\n",
    "                        \"group_size\": def_group,   # ê°•ì œ ì£¼ì…\n",
    "                        \"price_adult\": price,\n",
    "                        \"status\": \"available\"\n",
    "                    })\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        return rows\n",
    "\n",
    "    def _merge(self, prod, price):\n",
    "        # ë°©ì–´ ì½”ë“œ: prodê°€ ì—¬ì „íˆ dictê°€ ì•„ë‹ˆë©´ ë¹ˆ dictë¡œ ì´ˆê¸°í™”\n",
    "        if not isinstance(prod, dict): prod = {}\n",
    "        if not isinstance(price, dict): price = {}\n",
    "\n",
    "        # 1. ê°€ê²© ë¦¬ìŠ¤íŠ¸ ë³‘í•©\n",
    "        prod[\"price_info\"] = price.get(\"price_info\", [])\n",
    "\n",
    "        # 2. ê¸€ë¡œë²Œ ì •ì±… ì—…ë°ì´íŠ¸\n",
    "        updates = price.get(\"global_updates\", {})\n",
    "\n",
    "        # basic_info í‚¤ê°€ ì—†ìœ¼ë©´ ìƒì„± (ë°©ì–´ ì½”ë“œ)\n",
    "        if \"basic_info\" not in prod: prod[\"basic_info\"] = {}\n",
    "        if \"policies\" not in prod: prod[\"policies\"] = {}\n",
    "\n",
    "        if updates.get(\"is_vat_included\") is not None:\n",
    "            prod[\"basic_info\"][\"is_vat_included\"] = updates[\"is_vat_included\"]\n",
    "\n",
    "        if updates.get(\"cancellation_refund\"):\n",
    "            prod[\"policies\"][\"cancellation_refund\"] = updates[\"cancellation_refund\"]\n",
    "\n",
    "        return prod\n",
    "\n",
    "# --- ì‹¤í–‰ ---\n",
    "if __name__ == \"__main__\":\n",
    "    # íŒŒì¼ ê²½ë¡œ ì„¤ì • (í…ŒìŠ¤íŠ¸ìš©)\n",
    "    PRODUCT_FILE = \"../ERP í•„ìš”í•œ ë°ì´í„°/1. ëœë“œì‚¬í•œí…Œ ë°›ì€ ìƒí’ˆ_ì™„ë£Œ/ìƒí’ˆ/test.pdf\"\n",
    "    PRICE_FILE = \"../ERP í•„ìš”í•œ ë°ì´í„°/1. ëœë“œì‚¬í•œí…Œ ë°›ì€ ìƒí’ˆ_ì™„ë£Œ/ê°€ê²©í‘œ/text.pdf\"\n",
    "\n",
    "    if os.path.exists(PRODUCT_FILE):\n",
    "        ai = TravelVisionAI()\n",
    "\n",
    "        # Case 1: ìƒí’ˆ+ê°€ê²© íŒŒì¼ ë‘˜ ë‹¤ ìˆì„ ë•Œ\n",
    "        if os.path.exists(PRICE_FILE):\n",
    "            print(\">>> [TEST] ìƒí’ˆ + ê°€ê²© íŒŒì¼ ëª¨ë“œ\")\n",
    "            result = ai.process(PRODUCT_FILE, PRICE_FILE)\n",
    "\n",
    "        # Case 2: ìƒí’ˆ íŒŒì¼ë§Œ ìˆì„ ë•Œ (ìƒí’ˆ íŒŒì¼ ë‚´ì— ê°€ê²©í‘œê°€ ìˆë‹¤ê³  ê°€ì •)\n",
    "        else:\n",
    "            print(\">>> [TEST] í†µí•© íŒŒì¼ ëª¨ë“œ (ê°€ê²© íŒŒì¼ ì—†ìŒ)\")\n",
    "            result = ai.process(PRODUCT_FILE, None)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ğŸ¯ FINAL JSON RESULT\")\n",
    "        print(\"=\"*50)\n",
    "        print(json.dumps(result, indent=2, ensure_ascii=False))\n",
    "    else:\n",
    "        print(\"âŒ ìƒí’ˆ íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63fb1e-fa30-4b04-a5a2-c46287ab302b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848393f7-3693-44d4-8798-7abcf701c6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a7eb06-db29-4f70-af19-a810978d08e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627a87ef-2a7d-4939-ac6b-095c4fba006c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6dc679-7479-49c9-aba6-02876cde9cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a7263-5c38-4709-9631-8651199731f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e770a5-c14c-45e3-a52c-142d91ef427c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eed48a-42a7-49a7-a701-fb61d8e3e0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb792d2-f4d3-45bc-8fcd-104df86f7d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e593db32-7fe9-4f6e-9ef4-6e33e4bb131d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd40677-2890-44e9-8b40-8890bd58e2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687923c3-8cad-408b-8726-c7f080f6d2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b89e355-a705-470a-9339-2354eb396cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe6a29b-f5d9-4efb-a543-b67a742ccd53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da65708a-179a-44b3-9a7a-4586935df686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd68a3f-83ec-460d-8eec-83711f0438be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42066afa-f9af-4cf4-a5b9-63f377b68567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97cef6c-2325-49ab-b419-bc4eeb95e903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19dc0cf-39ac-4905-9b6a-edd697e7f6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eef206e-3b6b-429c-8ac4-4056ddc1dd5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d423f4ae-30f2-494b-98cb-e005b888a5db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1eb153-9fd8-4f0c-b64b-a4ad795009ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855f59f0-68f2-4e73-92ab-0609dca1ea01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (travel)",
   "language": "python",
   "name": "travel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
